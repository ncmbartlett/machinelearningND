{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train a Quadcopter How to Fly\n",
    "\n",
    "Design an agent to fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! \n",
    "\n",
    "Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym, os, sys, random, copy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import namedtuple, deque, defaultdict\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.regularizers import l2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    \"\"\"Initialize an Agent object.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        env (obj): environment to use, for example an OpenAI gym environment\n",
    "        random_seed (int): random seed\n",
    "        max_eps (int): maximum number of episodes to run\n",
    "        max_steps (int): maximum number of steps to take per episode if there is no hardcoded episode terminator\n",
    "        train_every (str or int): set to 'step', 'episode', or int to train after every step, episode, or n number of steps\n",
    "        decay_noise (str): program to decay noise. set to False, 'exp', 'linear', or 'success'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, env_type, random_seed, max_eps, max_steps, train_every, decay_noise):\n",
    "        self.env = env\n",
    "        self.env_type = env_type # Either 'openai' or 'copter'\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.action_low = env.action_space.low[0]\n",
    "        self.action_high = env.action_space.high[0]\n",
    "        self.seed = random_seed\n",
    "        self.ep_num = 0  # Counter for episode number\n",
    "        self.step_num = 1  # Counter for step number\n",
    "        self.max_eps = max_eps  # Maximum number of episodes to run\n",
    "        self.max_steps = max_steps  # Maximum number of steps per episode\n",
    "        self.scale_state_vector = False  # True to apply min/max scaling on state vector\n",
    "        self.record = defaultdict(list)  # Stores records of all states, actions, and rewards\n",
    "        self.train_every = train_every  # Training frequency: 'step', 'episode', or int value (every n steps).\n",
    "        self.decay_noise = decay_noise  # Set to False, 'exp', 'linear', or 'success' (default is False)\n",
    "\n",
    "        # Actor (Policy) Model\n",
    "        self.actor_local = Actor(state_size=self.state_size,\n",
    "                                 action_size=self.action_size,\n",
    "                                 action_low=self.action_low,\n",
    "                                 action_high=self.action_high,\n",
    "                                 seed=random_seed)\n",
    "\n",
    "        self.actor_target = Actor(state_size=self.state_size,\n",
    "                                  action_size=self.action_size,\n",
    "                                  action_low=self.action_low,\n",
    "                                  action_high=self.action_high,\n",
    "                                  seed=random_seed)\n",
    "\n",
    "        # Critic (Value) Model\n",
    "        self.critic_local = Critic(state_size=self.state_size,\n",
    "                                   action_size=self.action_size,\n",
    "                                   seed=random_seed)\n",
    "\n",
    "        self.critic_target = Critic(state_size=self.state_size,\n",
    "                                    action_size=self.action_size,\n",
    "                                    seed=random_seed)\n",
    "\n",
    "        # Initialize target model parameters with local model parameters\n",
    "        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n",
    "        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n",
    "\n",
    "        # Noise process\n",
    "        self.exploration_mu = 0\n",
    "        self.exploration_theta = 0.15\n",
    "        self.exploration_sigma = 0.2\n",
    "\n",
    "        self.initial_noise_scale = 1\n",
    "        self.noise_scale = self.initial_noise_scale\n",
    "        self.noise_decay_rate = 0.99\n",
    "        self.noise = OUNoise(self.action_size,\n",
    "                             random_seed,\n",
    "                             self.exploration_mu,\n",
    "                             self.exploration_theta,\n",
    "                             self.exploration_sigma)\n",
    "\n",
    "        # Replay memory\n",
    "        self.buffer_size = int(1e5)  # replay buffer size\n",
    "        self.batch_size = 128  # minibatch size\n",
    "        self.sparse_reward_weight = False  # weighting factor for prioritized replay\n",
    "        self.subsample_size = None  # size of subsample to use for prioritized replay\n",
    "        self.memory = ReplayBuffer(self.buffer_size,\n",
    "                                   self.batch_size,\n",
    "                                   self.sparse_reward_weight,\n",
    "                                   random_seed,\n",
    "                                   self.subsample_size)\n",
    "\n",
    "        # Algorithm parameters\n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.tau = 0.001  # for soft update of target parameters\n",
    "        \n",
    "    def reset(self, env, env_type, random_seed, max_eps, max_steps, train_every, decay_noise):\n",
    "        \"\"\"Reset agent to its initial state.\"\"\"\n",
    "        \n",
    "        self.__init__(env, env_type, random_seed, max_eps, max_steps, train_every, decay_noise)\n",
    "        \n",
    "    def reset_episode(self):\n",
    "        self.noise.reset()  # reset the OUNoise\n",
    "        self.ep_num += 1  # increment the episode counter by 1\n",
    "        self.step_num = 1  # reset the step counter to 1\n",
    "\n",
    "    def increment_step(self):\n",
    "        \"\"\"Increment the step counter by 1.\"\"\"\n",
    "        self.step_num += 1\n",
    "\n",
    "    def decay_noise_scale(self, done):\n",
    "        \"\"\"Decay the noise scale according to a specified program. This is called by the step method.\n",
    "        Linear decay scales the action by p = ep_num/max_eps and the noise by 1 - p. Exponential decay\n",
    "        reduces initial_noise_scale by noise_decay_rate^ep_num. The last option is to decay when there\n",
    "        is a succesful episode. Here a success is defined as the episode terminating before max_steps\n",
    "        is reached (i.e., if the agent reaches a goal), so it doesn't work for say Pendulum, but works\n",
    "        for MountainCar. But this can be easily adapted for other defs of success.\"\"\"\n",
    "\n",
    "        # Linear decay\n",
    "        if self.decay_noise == 'linear':\n",
    "            p = self.ep_num / self.max_eps\n",
    "            self.noise_scale = 1 - p\n",
    "\n",
    "        # Exponential decay\n",
    "        if self.decay_noise == 'exp':\n",
    "            self.noise_scale = self.initial_noise_scale * (self.noise_decay_rate ** self.ep_num)\n",
    "\n",
    "        # Decay upon successful episode\n",
    "        if self.decay_noise == 'success':\n",
    "            if done and (self.step_num < self.max_steps):\n",
    "                self.noise_scale *= self.noise_decay_rate\n",
    "\n",
    "    def store_record(self, state, action, reward, done):\n",
    "        \"\"\"Store experience in record for analysis.\"\"\"\n",
    "\n",
    "        for i in range(len(state)):\n",
    "            self.record['State ' + str(i)].append(state[i])\n",
    "        for i in range(len(action)):\n",
    "            self.record['Action ' + str(i)].append(action[i])\n",
    "        self.record['Reward'].append(reward)\n",
    "        self.record['Done'].append(done)\n",
    "        self.record['Noise scale'].append(self.noise_scale)\n",
    "        self.record['Episode'].append(self.ep_num)\n",
    "        self.record['Step'].append(self.step_num)\n",
    "        \n",
    "        if self.env_type == 'copter':\n",
    "            dims = ['x', 'y', 'z']\n",
    "            for i in range(len(self.env.sim.v)):\n",
    "                self.record['Lin_v ' + dims[i]].append(self.env.sim.v[i])\n",
    "            for i in range(len(self.env.sim.angular_v)):\n",
    "                self.record['Ang_v ' + str(i)].append(self.env.sim.angular_v[i])\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience/reward in record for later analysis (do this before scaling state vector)\n",
    "        self.store_record(state, action, reward, done)\n",
    "        \n",
    "        # Save experience/reward in Replay Buffer\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Train depending on frequency desired (every ep, every step, every n steps)\n",
    "        if self.train_every == 'episode':\n",
    "            if done or (self.step_num == self.max_steps):\n",
    "                # Learn, if enough samples are available in memory\n",
    "                if len(self.memory) > self.batch_size:\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences)\n",
    "        elif self.train_every == 'step':\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "        elif type(self.train_every) == int:\n",
    "            if self.step_num % self.train_every == 0:\n",
    "                if len(self.memory) > self.batch_size:\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences)\n",
    "\n",
    "        # Increment step count by 1\n",
    "        self.increment_step()\n",
    "\n",
    "        # Decay noise scale\n",
    "        self.decay_noise_scale(done)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state(s) as per current policy.\"\"\"\n",
    "\n",
    "        # Scale the state vector if desired.\n",
    "        if self.scale_state_vector:\n",
    "            state = self.scale_state(state)\n",
    "\n",
    "        state = np.reshape(state, [-1, self.state_size])\n",
    "        action = self.actor_local.model.predict(state)[0]\n",
    "\n",
    "        # Decay noise if desired (see decay_noise_scale method).\n",
    "        if self.decay_noise == 'linear':\n",
    "            action = action * (1 - self.noise_scale) + (self.noise_scale * self.noise.sample())\n",
    "        else:\n",
    "            action += self.noise_scale * self.noise.sample()\n",
    "\n",
    "        return list(np.clip(action, self.action_low, self.action_high))\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = rewards + γ * critic_target(next_states, actor_target(next_states))\n",
    "        \"\"\"\n",
    "        # Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        if self.scale_state_vector:\n",
    "            states = self.scale_state(states)\n",
    "        actions = np.array([e.action for e in experiences if e is not None]).astype(np.float32).reshape(-1,\n",
    "                                                                                                        self.action_size)\n",
    "        rewards = np.array([e.reward for e in experiences if e is not None]).astype(np.float32).reshape(-1, 1)\n",
    "        dones = np.array([e.done for e in experiences if e is not None]).astype(np.uint8).reshape(-1, 1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "        if self.scale_state_vector:\n",
    "            next_states = self.scale_state(next_states)\n",
    "\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target.model.predict_on_batch(next_states)\n",
    "        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n",
    "\n",
    "        # Compute Q targets for current states and train critic model (local)\n",
    "        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n",
    "        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)\n",
    "\n",
    "        # Train actor model (local)\n",
    "        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]),\n",
    "                                      (-1, self.action_size))\n",
    "        self.actor_local.train_fn([states, action_gradients, 1])  # custom training function\n",
    "\n",
    "        # Soft-update target models\n",
    "        self.soft_update(self.critic_local.model, self.critic_target.model)\n",
    "        self.soft_update(self.actor_local.model, self.actor_target.model)\n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\"\"\"\n",
    "        local_weights = np.array(local_model.get_weights())\n",
    "        target_weights = np.array(target_model.get_weights())\n",
    "\n",
    "        assert len(local_weights) == len(target_weights), \"Local and target model parameters must have the same size\"\n",
    "\n",
    "        new_weights = self.tau * local_weights + (1 - self.tau) * target_weights\n",
    "        target_model.set_weights(new_weights)\n",
    "\n",
    "    def scale_state(self, state):\n",
    "        \"\"\"Min-max scaler for state vector.\"\"\"\n",
    "\n",
    "        mins = np.array([self.env.observation_space.low[i] for i in range(self.env.observation_space.shape[0])])\n",
    "        maxes = np.array([self.env.observation_space.high[i] for i in range(self.env.observation_space.shape[0])])\n",
    "\n",
    "        return np.divide(state - mins, maxes - mins)\n",
    "    \n",
    "    def train_agent(self, threshold=False):\n",
    "        \"\"\"Train agent in environment. Environment can be either 'copter' or 'openai'.\"\"\"\n",
    "        \n",
    "        scores = []\n",
    "        scores_deque = deque(maxlen=10)\n",
    "        \n",
    "        if threshold:\n",
    "            trained = False\n",
    "            while trained == False:\n",
    "                for _ in range(1, self.max_eps + 1):\n",
    "                    state = self.env.reset() # start a new episode\n",
    "                    self.reset_episode()\n",
    "                    score = 0\n",
    "                    while True:\n",
    "                        action = self.act(state)\n",
    "                        if self.env_type == 'copter':\n",
    "                            next_state, reward, done = self.env.step(action)\n",
    "                        elif self.env_type == 'openai':\n",
    "                            next_state, reward, done, _ = self.env.step(action)\n",
    "                        self.step(state, action, reward, next_state, done)\n",
    "                        state = next_state\n",
    "                        score += reward\n",
    "                        if done:\n",
    "                            scores.append(score)\n",
    "                            scores_deque.append(score)\n",
    "                            if self.env_type == 'copter':\n",
    "                                print(\"Episode {}/{}, score = {:.1f}, Noise = {:.3f}, Time = {:.2f}, Final State: ({:.2f}, {:.2f}, {:.2f})\"\n",
    "                                      .format(self.ep_num, self.max_eps, score, self.noise_scale, \n",
    "                                              self.env.sim.time, state[0], state[1], state[2]))\n",
    "                            elif self.env_type == 'openai':\n",
    "                                print(\"Episode {}/{}, score = {:.1f}, Noise = {:.3f}\"\n",
    "                                      .format(self.ep_num,  self.max_eps, score, self.noise_scale))\n",
    "                            break\n",
    "\n",
    "                    if (self.ep_num % 10 == 0):\n",
    "                        if (np.mean(scores_deque) < threshold):\n",
    "                            print('Agent failed to learn or got stuck in local minimum: Resetting networks and trying again.')\n",
    "                            self.reset(self.env, self.env_type, self.seed, max_eps=self.max_eps, max_steps=self.max_steps, \n",
    "                                       train_every=self.train_every, decay_noise=self.decay_noise)\n",
    "                            break\n",
    "                    if self.ep_num == self.max_eps:\n",
    "                        print('Agent trained succesfully!')\n",
    "                        trained = True\n",
    "                        break\n",
    "                            \n",
    "        else:\n",
    "            for _ in range(1, self.max_eps + 1):\n",
    "                    state = self.env.reset() # start a new episode\n",
    "                    self.reset_episode()\n",
    "                    score = 0\n",
    "                    while True:\n",
    "                        action = self.act(state)\n",
    "                        if self.env_type == 'copter':\n",
    "                            next_state, reward, done = self.env.step(action)\n",
    "                        elif self.env_type == 'openai':\n",
    "                            next_state, reward, done, _ = self.env.step(action)\n",
    "                        self.step(state, action, reward, next_state, done)\n",
    "                        state = next_state\n",
    "                        score += reward\n",
    "                        if done:\n",
    "                            scores.append(score)\n",
    "                            if self.env_type == 'copter':\n",
    "                                print(\"Episode {}/{}, score = {:.1f}, Noise = {:.3f}, Time = {:.2f}, Final State: ({:.2f}, {:.2f}, {:.2f})\"\n",
    "                                      .format(self.ep_num,self.max_eps, score, self.noise_scale, \n",
    "                                              self.env.sim.time, state[0], state[1], state[2]))\n",
    "                            elif self.env_type == 'openai':\n",
    "                                print(\"Episode {}/{}, score = {:.1f}, Noise = {:.3f}\"\n",
    "                                      .format(self.ep_num,  self.max_eps, score, self.noise_scale))\n",
    "                            break\n",
    "                    \n",
    "        return scores\n",
    "    \n",
    "    def watch_agent(self):\n",
    "        \"\"\"Watch a trained agent's performance in an environment.\"\"\"\n",
    "        \n",
    "        self.ep_num = 0\n",
    "        \n",
    "        scores = []\n",
    "        for _ in range(1, self.max_eps + 1):\n",
    "            state = self.env.reset() # start a new episode\n",
    "            self.reset_episode()\n",
    "            score = 0\n",
    "            while True:\n",
    "                action = self.act(state)\n",
    "                if self.env_type == 'copter':\n",
    "                    state, reward, done = self.env.step(action)\n",
    "                elif self.env_type == 'openai':\n",
    "                    state, reward, done, _ = self.env.step(action)\n",
    "                    if self.ep_num % 10 == 0:\n",
    "                        self.env.render()\n",
    "                score += reward\n",
    "                if done:\n",
    "                    scores.append(score)\n",
    "                    if self.env_type == 'copter':\n",
    "                        print(\"Episode {}/{}, score = {:.1f}, Noise = {:.3f}, Time = {:.2f}, Final State: ({:.2f}, {:.2f}, {:.2f})\"\n",
    "                              .format(self.ep_num, self.max_eps, score, self.noise_scale, \n",
    "                                      self.env.sim.time, state[0], state[1], state[2]))\n",
    "                    elif self.env_type == 'openai':\n",
    "                        print(\"Episode {}/{}, score = {:.1f}, Noise = {:.3f}\"\n",
    "                              .format(self.ep_num, self.max_eps, score, self.noise_scale))\n",
    "                    break\n",
    "                    \n",
    "        return scores\n",
    "        \n",
    "        \n",
    "class Actor:\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, action_low, action_high, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            action_low (array): Min value of each action dimension\n",
    "            action_high (array): Max value of each action dimension\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "        self.seed = seed\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        # Define input layer (states)\n",
    "        states = layers.Input(shape=(self.state_size,), name='states')\n",
    "\n",
    "        # Add hidden layers\n",
    "        kernel_init_hl1 = RandomUniform(minval=-1/np.sqrt(400), maxval=1/np.sqrt(400), seed=self.seed)\n",
    "        net = layers.Dense(units=400, activation='relu', kernel_initializer=kernel_init_hl1)(states)\n",
    "        kernel_init_hl2 = RandomUniform(minval=-1 / np.sqrt(300), maxval=1 / np.sqrt(300), seed=self.seed)\n",
    "        net = layers.Dense(units=300, activation='relu', kernel_initializer=kernel_init_hl2)(net)\n",
    "        kernel_init_hl3 = RandomUniform(minval=-1 / np.sqrt(200), maxval=1 / np.sqrt(200), seed=self.seed)\n",
    "        net = layers.Dense(units=200, activation='relu', kernel_initializer=kernel_init_hl3)(net)\n",
    "\n",
    "        #  Add final output layer with sigmoid activation\n",
    "        kernel_init_out = RandomUniform(minval=-3e-3, maxval=3e-3, seed=self.seed)\n",
    "        raw_actions = layers.Dense(units=self.action_size, activation='sigmoid', kernel_initializer=kernel_init_out,\n",
    "                                   name='raw_actions')(net)\n",
    "\n",
    "        # Scale [0, 1] output for each action dimension to proper range\n",
    "        actions = layers.Lambda(lambda x: (x * self.action_range) + self.action_low, name='actions')(raw_actions)\n",
    "\n",
    "        # Create Keras model\n",
    "        self.model = models.Model(inputs=states, outputs=actions)\n",
    "\n",
    "        # Define loss function using action value (Q value) gradients\n",
    "        action_gradients = layers.Input(shape=(self.action_size,))\n",
    "        loss = K.mean(-action_gradients * actions)\n",
    "\n",
    "        # Define optimizer and training function\n",
    "        optimizer = optimizers.Adam(lr=1e-3)\n",
    "        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
    "        self.train_fn = K.function(\n",
    "            inputs=[self.model.input, action_gradients, K.learning_phase()],\n",
    "            outputs=[],\n",
    "            updates=updates_op)\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = seed\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        # Define input layers\n",
    "        states = layers.Input(shape=(self.state_size,), name='states')\n",
    "        actions = layers.Input(shape=(self.action_size,), name='actions')\n",
    "\n",
    "        # Add hidden layer(s) for state pathway\n",
    "        kernel_init_hl1 = RandomUniform(minval=-1/np.sqrt(400), maxval=1/np.sqrt(400), seed=self.seed)\n",
    "        net_states = layers.Dense(units=400, activation='relu', kernel_initializer=kernel_init_hl1)(states)\n",
    "\n",
    "        # Combine state and action pathways\n",
    "        state_action = layers.Concatenate()([net_states, actions])\n",
    "\n",
    "        # Add hidden layer(s) for combined state/action pathway\n",
    "        kernel_init_hl2 = RandomUniform(minval=-1/np.sqrt(300), maxval=1/np.sqrt(300), seed=self.seed)\n",
    "        net = layers.Dense(units=300, activation='relu', kernel_initializer=kernel_init_hl2)(state_action)\n",
    "        kernel_init_hl3 = RandomUniform(minval=-1/np.sqrt(200), maxval=1/np.sqrt(200), seed=self.seed)\n",
    "        net = layers.Dense(units=200, activation='relu', kernel_initializer=kernel_init_hl3)(net)\n",
    "\n",
    "        # Add final output layer to produce action values (Q values)\n",
    "        kernel_init_out = RandomUniform(minval=-3e-3, maxval=3e-3, seed=self.seed)\n",
    "        Q_values = layers.Dense(units=1, name='q_values', kernel_initializer=kernel_init_out)(net)\n",
    "\n",
    "        # Create Keras model\n",
    "        self.model = models.Model(inputs=[states, actions], outputs=Q_values)\n",
    "\n",
    "        # Define optimizer and compile model for training with built-in loss function\n",
    "        optimizer = optimizers.Adam(lr=1e-2)\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Compute action gradients (derivative of Q values w.r.t. to actions)\n",
    "        action_gradients = K.gradients(Q_values, actions)\n",
    "\n",
    "        # Define an additional function to fetch action gradients (to be used by actor model)\n",
    "        self.get_action_gradients = K.function(\n",
    "            inputs=[*self.model.input, K.learning_phase()],\n",
    "            outputs=action_gradients)\n",
    "\n",
    "        \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, sparse_reward_weight, seed, subsample_size=None):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size: maximum size of buffer\n",
    "            batch_size: size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size             # mini-batch size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.sparse_reward_weight = sparse_reward_weight\n",
    "        self.seed = random.seed(seed)\n",
    "        self.subsample_size = subsample_size     # subsample size for priority replay if replay buffer is large\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        if self.sparse_reward_weight:\n",
    "            if self.sparse_reward_weight >= 1:\n",
    "                self.sparse_reward_weight = 1 - 1e-8\n",
    "            # Use a subsample of the memory if the memory is large to avoid slow performance\n",
    "            if self.subsample_size is not None:\n",
    "                if self.subsample_size > len(self.memory):  # This will only be True at the very beginning.\n",
    "                    sorted_mem = sorted(self.memory, key=lambda x: x.reward, reverse=True)\n",
    "                else:\n",
    "                    subsample = random.sample(self.memory, k=self.subsample_size)\n",
    "                    sorted_mem = sorted(subsample, key=lambda x: x.reward, reverse=True)\n",
    "                prob = np.array([self.sparse_reward_weight ** i for i in range(len(sorted_mem))])\n",
    "                prob /= np.sum(prob)\n",
    "                indices = np.random.choice(np.arange(len(self.memory)), replace=False, size=self.batch_size, p=prob)\n",
    "                sample_batch = np.array(sorted_mem)[indices]\n",
    "                sample_batch = [self.experience(state, action, reward, next_state, done) for state, action, reward, next_state, done in sample_batch]\n",
    "            else:\n",
    "                sorted_mem = sorted(self.memory, key=lambda x: x.reward, reverse=True)\n",
    "                prob = np.array([self.sparse_reward_weight ** i for i in range(len(sorted_mem))])\n",
    "                prob /= np.sum(prob)\n",
    "                indices = np.random.choice(np.arange(len(self.memory)), replace=False, size=self.batch_size, p=prob)\n",
    "                sample_batch = np.array(sorted_mem)[indices]\n",
    "                sample_batch = [self.experience(state, action, reward, next_state, done) for state, action, reward, next_state, done in sample_batch]\n",
    "            return sample_batch\n",
    "        else:\n",
    "            return random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu, theta, sigma):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from physics_sim import PhysicsSim\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "class Task():\n",
    "    \"\"\"Task (environment) that defines the goal and provides feedback to the agent. Model this like the OpenAI Gym\n",
    "    environments for compatibility with the same agents.\"\"\"\n",
    "\n",
    "    def __init__(self, init_pose=None, init_velocities=None, init_angle_velocities=None, runtime=5., target_pos=None):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "            init_pose: init position (x,y,z) of the quadcopter and the Euler angles phi (roll), theta (pitch), psi (yaw)\n",
    "            init_velocities: init velocity (vx, vy, vz) of the quadcopter\n",
    "            init_angle_velocities: init angular velocities in rad/s for each of the three Euler angles\n",
    "            runtime: time limit for each episode\n",
    "            target_pos: target/goal (x,y,z) position for the agent\n",
    "        \"\"\"\n",
    "\n",
    "        # Simulation\n",
    "        self.sim = PhysicsSim(init_pose, init_velocities, init_angle_velocities, runtime)\n",
    "        self.action_low = 0\n",
    "        self.action_high = 900\n",
    "        self.low_state = np.array([-150.0, -150.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        self.high_state = np.array([150.0, 150.0, 300.0, 2 * np.pi, 2 * np.pi, 2 * np.pi])\n",
    "        self.target_pos = target_pos if target_pos is not None else np.array([0., 0., 10.])\n",
    "        self.action_repeat = 2\n",
    "        self.state_size = self.action_repeat * 6\n",
    "        self.action_size = 4\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(low=np.repeat(self.low_state, 2),\n",
    "                                                high=np.repeat(self.high_state, 2),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "        self.action_space = gym.spaces.Box(low=self.action_low,\n",
    "                                           high=self.action_high,\n",
    "                                           shape=(4,),\n",
    "                                           dtype=np.float32)\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Uses current pose of sim to return reward.\"\"\"\n",
    "        delta_xy = euclidean(self.sim.pose[:2], self.target_pos[:2])\n",
    "        max_delta_xy = euclidean(self.low_state[:2], self.high_state[:2])\n",
    "        delta_z = self.sim.pose[2] - self.target_pos[2]\n",
    "        max_delta_z = 300\n",
    "        \n",
    "        distance_penalty = (delta_xy / max_delta_xy) - 2 * (delta_z / max_delta_z)\n",
    "        angular_penalty = (np.sum(self.sim.pose[3:]) / (6 * np.pi))\n",
    "        velocity_reward = 2 * self.sim.v[2]\n",
    "        velocity_penalty = abs(self.sim.v[1]) + abs(self.sim.v[0])\n",
    "\n",
    "        reward = (1. - distance_penalty - angular_penalty + velocity_reward - velocity_penalty) / 10\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Uses action (rotor speeds) to obtain next state, reward, done.\"\"\"\n",
    "\n",
    "        reward = 0\n",
    "        pose_all = []\n",
    "        for _ in range(self.action_repeat):\n",
    "            done = self.sim.next_timestep(action)  # update the sim pose and velocities\n",
    "            reward += self.get_reward()\n",
    "            # Extra reward for reaching target position\n",
    "            if self.sim.pose[2] >= self.target_pos[2]:\n",
    "                reward += 100\n",
    "                done = True\n",
    "                # Extra reward/penalty for getting within +/- 5 of the target position in the xy plane\n",
    "                if (self.sim.pose[0] - 5 < self.target_pos[0] < self.sim.pose[0] + 5) and (self.sim.pose[1] - 5 < self.target_pos[1] < self.sim.pose[1] + 5):\n",
    "                    reward += 100\n",
    "            # Extra penalty for running out of time\n",
    "            if self.sim.time > self.sim.runtime:\n",
    "                reward -= 100\n",
    "            # Extra penalty going out of bounds (crashing)\n",
    "            if np.any(self.sim.pose[:3] <= self.low_state[:3]) or np.any(self.sim.pose[:3] > self.high_state[:3]):\n",
    "                reward -= 100\n",
    "            pose_all.append(self.sim.pose)\n",
    "        next_state = np.concatenate(pose_all)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the sim to start a new episode.\"\"\"\n",
    "\n",
    "        self.sim.reset()\n",
    "        state = np.concatenate([self.sim.pose] * self.action_repeat) \n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nate/Documents/Python/MachineLearningND/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/50, score = -287.2, Noise = 1.000, Time = 5.04, Final State: (-15.27, 21.05, 19.47)\n",
      "Episode 2/50, score = -572.1, Noise = 1.000, Time = 3.00, Final State: (-41.35, 9.44, 0.00)\n",
      "Episode 3/50, score = -460.8, Noise = 1.000, Time = 2.76, Final State: (-25.91, 150.00, 70.94)\n",
      "Episode 4/50, score = -421.2, Noise = 1.000, Time = 2.52, Final State: (-8.47, 12.40, 0.00)\n",
      "Episode 5/50, score = -457.2, Noise = 1.000, Time = 1.72, Final State: (-2.09, 19.95, 0.00)\n",
      "Episode 6/50, score = 1110.1, Noise = 1.000, Time = 4.88, Final State: (26.98, -10.74, 129.71)\n",
      "Episode 7/50, score = 1289.7, Noise = 1.000, Time = 4.88, Final State: (-22.67, 3.64, 130.37)\n",
      "Episode 8/50, score = 1276.9, Noise = 1.000, Time = 4.76, Final State: (5.23, 1.28, 129.82)\n",
      "Episode 9/50, score = 1240.9, Noise = 1.000, Time = 4.76, Final State: (-12.36, -0.55, 129.83)\n",
      "Episode 10/50, score = 1263.6, Noise = 1.000, Time = 4.92, Final State: (-28.23, 2.55, 130.06)\n",
      "Episode 11/50, score = 1371.5, Noise = 1.000, Time = 4.72, Final State: (-2.05, 7.02, 130.10)\n",
      "Episode 12/50, score = 1222.2, Noise = 1.000, Time = 4.76, Final State: (5.19, 11.75, 129.52)\n",
      "Episode 13/50, score = 1194.7, Noise = 1.000, Time = 4.32, Final State: (21.28, 1.61, 129.44)\n",
      "Episode 14/50, score = 1300.2, Noise = 1.000, Time = 4.80, Final State: (9.26, 14.10, 130.51)\n",
      "Episode 15/50, score = 1154.6, Noise = 1.000, Time = 4.88, Final State: (-2.06, -25.94, 129.68)\n",
      "Episode 16/50, score = 1224.3, Noise = 1.000, Time = 4.80, Final State: (2.97, -13.08, 129.98)\n",
      "Episode 17/50, score = 1269.6, Noise = 1.000, Time = 4.76, Final State: (6.05, 1.04, 129.45)\n",
      "Episode 18/50, score = 1346.6, Noise = 1.000, Time = 4.80, Final State: (-11.07, -0.20, 130.47)\n",
      "Episode 19/50, score = 1282.6, Noise = 1.000, Time = 4.76, Final State: (-5.14, -0.40, 129.63)\n",
      "Episode 20/50, score = 1348.4, Noise = 1.000, Time = 4.80, Final State: (2.50, 11.49, 130.38)\n",
      "Episode 21/50, score = 1232.0, Noise = 1.000, Time = 4.72, Final State: (10.13, 5.61, 129.76)\n",
      "Episode 22/50, score = 1393.5, Noise = 1.000, Time = 4.76, Final State: (-2.33, 0.94, 129.62)\n",
      "Episode 23/50, score = 1386.2, Noise = 1.000, Time = 4.76, Final State: (2.67, 2.36, 129.60)\n",
      "Episode 24/50, score = 1346.7, Noise = 1.000, Time = 4.76, Final State: (10.52, -1.61, 130.05)\n",
      "Episode 25/50, score = 1290.9, Noise = 1.000, Time = 4.80, Final State: (7.03, -16.06, 130.14)\n",
      "Episode 26/50, score = 1171.0, Noise = 1.000, Time = 4.84, Final State: (-12.05, 17.25, 129.90)\n",
      "Episode 27/50, score = 1260.1, Noise = 1.000, Time = 4.76, Final State: (6.64, 2.28, 129.48)\n",
      "Episode 28/50, score = 1179.4, Noise = 1.000, Time = 4.84, Final State: (-20.00, -6.06, 129.85)\n",
      "Episode 29/50, score = 1241.4, Noise = 1.000, Time = 4.76, Final State: (4.46, 9.46, 129.74)\n",
      "Episode 30/50, score = 1349.8, Noise = 1.000, Time = 4.80, Final State: (-1.27, -11.36, 130.13)\n",
      "Episode 31/50, score = 1403.5, Noise = 1.000, Time = 4.76, Final State: (-0.48, -0.62, 129.66)\n",
      "Episode 32/50, score = 1235.4, Noise = 1.000, Time = 4.76, Final State: (4.60, 10.67, 129.89)\n",
      "Episode 33/50, score = 1246.0, Noise = 1.000, Time = 4.76, Final State: (9.81, 1.53, 129.66)\n",
      "Episode 34/50, score = 1297.9, Noise = 1.000, Time = 4.72, Final State: (16.28, 7.03, 130.20)\n",
      "Episode 35/50, score = 1310.9, Noise = 1.000, Time = 4.80, Final State: (-12.45, 9.92, 130.37)\n",
      "Episode 36/50, score = 1204.1, Noise = 1.000, Time = 4.64, Final State: (19.55, 2.02, 130.00)\n",
      "Episode 37/50, score = 1365.5, Noise = 1.000, Time = 4.76, Final State: (3.01, 6.84, 130.11)\n",
      "Episode 38/50, score = 1308.1, Noise = 1.000, Time = 4.84, Final State: (-18.98, 0.87, 130.32)\n",
      "Episode 39/50, score = 1289.3, Noise = 1.000, Time = 4.88, Final State: (-24.06, 0.73, 130.14)\n",
      "Episode 40/50, score = 1118.6, Noise = 1.000, Time = 4.88, Final State: (-24.19, 14.48, 129.56)\n",
      "Episode 41/50, score = 1310.0, Noise = 1.000, Time = 4.76, Final State: (14.62, -4.99, 130.30)\n",
      "Episode 42/50, score = 1379.1, Noise = 1.000, Time = 4.76, Final State: (1.31, 5.70, 130.04)\n",
      "Episode 43/50, score = 1374.3, Noise = 1.000, Time = 4.76, Final State: (-2.02, -4.30, 129.69)\n",
      "Episode 44/50, score = 1211.8, Noise = 1.000, Time = 4.80, Final State: (-4.25, -14.93, 129.60)\n",
      "Episode 45/50, score = 1344.9, Noise = 1.000, Time = 4.80, Final State: (10.40, 3.24, 130.15)\n",
      "Episode 46/50, score = 1367.8, Noise = 1.000, Time = 4.76, Final State: (5.66, 3.21, 130.11)\n",
      "Episode 47/50, score = 1236.3, Noise = 1.000, Time = 4.76, Final State: (-9.39, 5.68, 129.62)\n",
      "Episode 48/50, score = 1236.5, Noise = 1.000, Time = 4.80, Final State: (-14.29, 0.03, 129.79)\n",
      "Episode 49/50, score = 1195.7, Noise = 1.000, Time = 4.76, Final State: (11.65, 10.89, 129.74)\n",
      "Episode 50/50, score = 1087.6, Noise = 1.000, Time = 5.00, Final State: (-34.19, -9.84, 129.91)\n"
     ]
    }
   ],
   "source": [
    "target_pos = np.array([0., 0., 130.])\n",
    "task = Task(target_pos=target_pos)\n",
    "agent = DDPG(env=task, env_type='copter', random_seed=None, max_eps=50, max_steps=False, train_every='step', decay_noise=False)\n",
    "\n",
    "scores = agent.train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1209adda0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/50, score = 1312.5, Noise = 1.000, Time = 4.76, Final State: (-4.40, -15.52, 130.50)\n",
      "Episode 2/50, score = 1366.8, Noise = 1.000, Time = 4.72, Final State: (3.54, 4.75, 129.60)\n",
      "Episode 3/50, score = 1387.3, Noise = 1.000, Time = 4.76, Final State: (0.29, 1.68, 129.92)\n",
      "Episode 4/50, score = 1333.4, Noise = 1.000, Time = 4.80, Final State: (11.72, -3.14, 130.24)\n",
      "Episode 5/50, score = 1248.5, Noise = 1.000, Time = 4.76, Final State: (-4.23, 8.78, 129.58)\n",
      "Episode 6/50, score = 1378.9, Noise = 1.000, Time = 4.80, Final State: (6.85, 0.37, 130.54)\n",
      "Episode 7/50, score = 1389.4, Noise = 1.000, Time = 4.76, Final State: (-0.83, 2.30, 129.98)\n",
      "Episode 8/50, score = 1574.4, Noise = 1.000, Time = 4.76, Final State: (3.58, 2.10, 130.13)\n",
      "Episode 9/50, score = 1593.6, Noise = 1.000, Time = 4.76, Final State: (-2.80, 0.73, 130.49)\n",
      "Episode 10/50, score = 1342.7, Noise = 1.000, Time = 4.68, Final State: (0.24, 13.65, 130.60)\n",
      "Episode 11/50, score = 1265.9, Noise = 1.000, Time = 4.72, Final State: (0.28, 8.57, 129.67)\n",
      "Episode 12/50, score = 1272.1, Noise = 1.000, Time = 4.76, Final State: (6.25, 1.29, 129.75)\n",
      "Episode 13/50, score = 1267.4, Noise = 1.000, Time = 4.88, Final State: (-24.25, -4.48, 130.02)\n",
      "Episode 14/50, score = 1286.1, Noise = 1.000, Time = 4.84, Final State: (-5.61, -18.66, 130.38)\n",
      "Episode 15/50, score = 1152.6, Noise = 1.000, Time = 4.92, Final State: (-29.02, -2.02, 129.85)\n",
      "Episode 16/50, score = 1355.6, Noise = 1.000, Time = 4.80, Final State: (-9.35, -2.62, 130.26)\n",
      "Episode 17/50, score = 1240.4, Noise = 1.000, Time = 4.76, Final State: (6.30, 7.08, 129.46)\n",
      "Episode 18/50, score = 1343.6, Noise = 1.000, Time = 4.80, Final State: (1.91, -11.18, 130.17)\n",
      "Episode 19/50, score = 1336.0, Noise = 1.000, Time = 4.80, Final State: (13.13, -1.59, 130.20)\n",
      "Episode 20/50, score = 1342.4, Noise = 1.000, Time = 4.80, Final State: (-12.33, -1.85, 130.28)\n",
      "Episode 21/50, score = 1289.7, Noise = 1.000, Time = 4.84, Final State: (-21.80, 3.74, 130.09)\n",
      "Episode 22/50, score = 1179.3, Noise = 1.000, Time = 4.60, Final State: (12.73, 12.44, 129.41)\n",
      "Episode 23/50, score = 1264.7, Noise = 1.000, Time = 4.76, Final State: (-9.10, 0.52, 129.48)\n",
      "Episode 24/50, score = 1234.4, Noise = 1.000, Time = 4.64, Final State: (6.77, 7.01, 129.59)\n",
      "Episode 25/50, score = 1269.7, Noise = 1.000, Time = 4.76, Final State: (-0.24, -5.49, 130.00)\n",
      "Episode 26/50, score = 1193.1, Noise = 1.000, Time = 4.84, Final State: (-2.44, -20.68, 129.76)\n",
      "Episode 27/50, score = 1316.8, Noise = 1.000, Time = 4.80, Final State: (7.26, -10.49, 130.13)\n",
      "Episode 28/50, score = 1216.5, Noise = 1.000, Time = 4.80, Final State: (-12.23, -6.31, 129.90)\n",
      "Episode 29/50, score = 1231.0, Noise = 1.000, Time = 4.76, Final State: (-1.33, 15.26, 129.68)\n",
      "Episode 30/50, score = 1257.4, Noise = 1.000, Time = 4.76, Final State: (-3.25, 7.99, 129.48)\n",
      "Episode 31/50, score = 1177.0, Noise = 1.000, Time = 4.72, Final State: (9.90, 16.47, 129.65)\n",
      "Episode 32/50, score = 1379.3, Noise = 1.000, Time = 4.76, Final State: (3.60, 2.26, 129.83)\n",
      "Episode 33/50, score = 1397.6, Noise = 1.000, Time = 4.76, Final State: (-0.13, -0.26, 129.74)\n",
      "Episode 34/50, score = 1272.3, Noise = 1.000, Time = 4.76, Final State: (1.20, -5.63, 129.75)\n",
      "Episode 35/50, score = 1233.9, Noise = 1.000, Time = 4.60, Final State: (-5.66, 11.34, 129.81)\n",
      "Episode 36/50, score = 1359.6, Noise = 1.000, Time = 4.76, Final State: (4.95, -3.96, 129.57)\n",
      "Episode 37/50, score = 1362.1, Noise = 1.000, Time = 4.76, Final State: (-3.27, -6.84, 130.47)\n",
      "Episode 38/50, score = 1230.1, Noise = 1.000, Time = 4.76, Final State: (10.60, 5.10, 129.62)\n",
      "Episode 39/50, score = 1268.6, Noise = 1.000, Time = 4.76, Final State: (5.73, 1.86, 129.72)\n",
      "Episode 40/50, score = 1260.3, Noise = 1.000, Time = 4.76, Final State: (2.38, -5.73, 129.46)\n",
      "Episode 41/50, score = 1209.9, Noise = 1.000, Time = 4.80, Final State: (5.35, -13.10, 129.78)\n",
      "Episode 42/50, score = 1361.4, Noise = 1.000, Time = 4.76, Final State: (4.69, -5.37, 130.51)\n",
      "Episode 43/50, score = 1272.4, Noise = 1.000, Time = 4.76, Final State: (-6.06, 0.84, 129.49)\n",
      "Episode 44/50, score = 1058.5, Noise = 1.000, Time = 5.00, Final State: (13.06, -34.35, 129.63)\n",
      "Episode 45/50, score = 1135.1, Noise = 1.000, Time = 4.80, Final State: (9.02, -24.34, 129.88)\n",
      "Episode 46/50, score = 1236.4, Noise = 1.000, Time = 4.72, Final State: (5.58, 9.53, 129.98)\n",
      "Episode 47/50, score = 1229.7, Noise = 1.000, Time = 4.56, Final State: (1.17, 13.83, 129.46)\n",
      "Episode 48/50, score = 1355.7, Noise = 1.000, Time = 4.80, Final State: (-10.95, -1.63, 130.44)\n",
      "Episode 49/50, score = 1166.9, Noise = 1.000, Time = 4.84, Final State: (4.91, -22.05, 129.87)\n",
      "Episode 50/50, score = 1229.7, Noise = 1.000, Time = 4.76, Final State: (-4.68, -8.90, 129.46)\n"
     ]
    }
   ],
   "source": [
    "agent.watch_agent();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the Task, Design the Agent, and Train Your Agent!\n",
    "\n",
    "Amend `task.py` to specify a task of your choosing.  If you're unsure what kind of task to specify, you may like to teach your quadcopter to takeoff, hover in place, land softly, or reach a target pose.  \n",
    "\n",
    "After specifying your task, use the sample agent in `agents/policy_search.py` as a template to define your own agent in `agents/agent.py`.  You can borrow whatever you need from the sample agent, including ideas on how you might modularize your code (using helper methods like `act()`, `learn()`, `reset_episode()`, etc.).\n",
    "\n",
    "Note that it is **highly unlikely** that the first agent and task that you specify will learn well.  You will likely have to tweak various hyperparameters and the reward function for your task until you arrive at reasonably good behavior.\n",
    "\n",
    "As you develop your agent, it's important to keep an eye on how it's performing. Use the code above as inspiration to build in a mechanism to log/save the total rewards obtained in each episode to file.  If the episode rewards are gradually increasing, this is an indication that your agent is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Train your agent here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot the Rewards\n",
    "\n",
    "Once you are satisfied with your performance, plot the episode rewards, either from a single run, or averaged over multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Plot the rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflections\n",
    "\n",
    "**Question 1**: Describe the task that you specified in `task.py`.  How did you design the reward function?\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Discuss your agent briefly, using the following questions as a guide:\n",
    "\n",
    "- What learning algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Using the episode rewards plot, discuss how the agent learned over time.\n",
    "\n",
    "- Was it an easy task to learn or hard?\n",
    "- Was there a gradual learning curve, or an aha moment?\n",
    "- How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, plotting, specifying the task, etc.)\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**Answer**:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
